{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 10:28:29.408851: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-15 10:28:30.826528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-15 10:28:30.826576: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-15 10:28:40.555169: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-15 10:28:40.555871: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-15 10:28:40.555903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#tipicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#para la creacion de carpetas\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "#para el test-train split\n",
    "import splitfolders #(necesita pip install split-folders)\n",
    "\n",
    "#datagen\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#modelos\n",
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### primero tenemos q separar las imagenes en carpetas segun su master class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967/3561016237.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df_styles = pd.read_csv(path_csv, error_bad_lines = False)\n"
     ]
    }
   ],
   "source": [
    "#importamos el df con las labels\n",
    "path_csv = \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/df1.csv\"\n",
    "df_styles = pd.read_csv(path_csv, error_bad_lines = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creamos los filenames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos la lista de las labels\n",
    "labels= np.unique(df_styles.masterCategory).tolist()\n",
    "\n",
    "#guardamos la lista de los ids \n",
    "list_id = list(df_styles.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en files guardamos los path a cada foto en files y el nombre con .jpg en images\n",
    "images = []\n",
    "for file in list_id:\n",
    "    images.append(str(file) + \".jpg\")\n",
    "    \n",
    "path_img = \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/df1\"\n",
    "files = []\n",
    "for i in images:\n",
    "    files.append((os.path.join(path_img, i)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en un df guardamos el id, master category y agregamos el nombre de la imagen con jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5408/1087487744.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df = df_styles.drop(df_styles.columns.difference(['id', 'masterCategory']),1)\n"
     ]
    }
   ],
   "source": [
    "df = df_styles.drop(df_styles.columns.difference(['id', 'masterCategory']),1)\n",
    "df['name'] = images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creamos una variable con los nombres de las clases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df.sort_values('masterCategory')\n",
    "class_names = np.unique(df_styles.masterCategory).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### esto crea las carpetas vacias segun master category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in class_names:\n",
    "    os.makedirs(os.path.join(\"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/data\",i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in class_names:     # I ->  class label names\n",
    "    for c in list(df[df['masterCategory']== i]['name']):    # c  ->  individual image \n",
    "    # Creating path to the image\n",
    "        get_image = os.path.join(\"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/df1\",str(c))\n",
    "        if not os.path.exists(\"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/data/\"+i+\"/\"+c):\n",
    "            move = shutil.copy(get_image,\"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/data/\"+i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ahora hacemos la division de train test de las carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primero creamos la carpeta donde estara nuestra data spliteada\n",
    "# os.makedirs(\"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/data/\"\n",
    "output_folder = \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with a ratio.\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio(input_folder, output=output_folder,\n",
    "                    seed=42,\n",
    "                    ratio=(.7, .2, .1), #usamos 70% train, 20% val, 10% test \n",
    "                    group_prefix=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ahora que tenemos en subcarpetas todo probamos el image data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13997 images belonging to 7 classes.\n",
      "Found 3998 images belonging to 7 classes.\n",
      "Found 2005 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# iniciamos el gen escalando para que el input este entre 0 y 1\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#creamos los generadores con batch de 30 fotos con el dir correpondiente\n",
    "train_dir = \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/split/train\"\n",
    "val_dir = \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/split/val\"\n",
    "test_dir = \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/split/test\"\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                train_dir,\n",
    "                                target_size =(480, 360), # target_size = input image size\n",
    "                                batch_size = 5,\n",
    "                                class_mode ='categorical')\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "                                val_dir,\n",
    "                                target_size =(480, 360), # target_size = input image size\n",
    "                                batch_size = 5,\n",
    "                                class_mode ='categorical')\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "                                test_dir,\n",
    "                                target_size =(480, 360), # target_size = input image size\n",
    "                                batch_size = 5,\n",
    "                                class_mode ='categorical')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ahora creamos el modelo 1 para master category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_own_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(480,360,3), name='input'))\n",
    "    model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "    model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "    model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "                    optimizer = 'adam',\n",
    "                    metrics = 'accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-14 16:24:30.185909: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-14 16:24:30.187269: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-14 16:24:30.188216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-HHE71063): /proc/driver/nvidia/version does not exist\n",
      "2023-01-14 16:24:30.191014: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 471, 351, 16)      4816      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 157, 117, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 150, 110, 32)      32800     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 50, 36, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 45, 31, 32)        36896     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 15, 10, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4800)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               480100    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 707       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 555,319\n",
      "Trainable params: 555,319\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_own_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21765/2842717622.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 63s 4s/step - loss: 1.1176 - accuracy: 0.5600 - val_loss: 1.1697 - val_accuracy: 0.4680\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 58s 4s/step - loss: 1.0920 - accuracy: 0.5200 - val_loss: 1.0160 - val_accuracy: 0.4920\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 62s 4s/step - loss: 0.9587 - accuracy: 0.6667 - val_loss: 0.7936 - val_accuracy: 0.7120\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 71s 5s/step - loss: 0.8238 - accuracy: 0.7600 - val_loss: 0.5606 - val_accuracy: 0.7920\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 70s 5s/step - loss: 0.6407 - accuracy: 0.8533 - val_loss: 0.5868 - val_accuracy: 0.8040\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 59s 4s/step - loss: 0.5843 - accuracy: 0.7467 - val_loss: 0.9110 - val_accuracy: 0.6280\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 56s 4s/step - loss: 0.8882 - accuracy: 0.6667 - val_loss: 0.7313 - val_accuracy: 0.7520\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 68s 5s/step - loss: 0.6811 - accuracy: 0.7067 - val_loss: 0.5480 - val_accuracy: 0.8160\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 84s 6s/step - loss: 0.4389 - accuracy: 0.8400 - val_loss: 0.6636 - val_accuracy: 0.7920\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.6079 - accuracy: 0.8267 - val_loss: 0.5083 - val_accuracy: 0.8000\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 112s 8s/step - loss: 0.4415 - accuracy: 0.8667 - val_loss: 0.5021 - val_accuracy: 0.7920\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 75s 5s/step - loss: 0.2816 - accuracy: 0.8933 - val_loss: 0.4968 - val_accuracy: 0.8480\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 70s 5s/step - loss: 0.5221 - accuracy: 0.8133 - val_loss: 0.3376 - val_accuracy: 0.8880\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 66s 5s/step - loss: 0.4541 - accuracy: 0.9200 - val_loss: 0.5080 - val_accuracy: 0.8400\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 74s 5s/step - loss: 0.5540 - accuracy: 0.8267 - val_loss: 0.3236 - val_accuracy: 0.8920\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 71s 5s/step - loss: 0.5555 - accuracy: 0.8000 - val_loss: 0.4703 - val_accuracy: 0.8400\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.2597 - accuracy: 0.8800 - val_loss: 0.4526 - val_accuracy: 0.8640\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 66s 4s/step - loss: 0.3278 - accuracy: 0.8800 - val_loss: 0.4212 - val_accuracy: 0.8720\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 69s 5s/step - loss: 0.6031 - accuracy: 0.8000 - val_loss: 0.4934 - val_accuracy: 0.8240\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 88s 6s/step - loss: 0.5059 - accuracy: 0.8533 - val_loss: 0.3561 - val_accuracy: 0.8760\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "             train_generator,\n",
    "             steps_per_epoch = 15,\n",
    "             epochs = 30,\n",
    "             callbacks = EarlyStopping(patience = 5, restore_best_weights=True),\n",
    "             validation_data = val_generator,\n",
    "             validation_steps = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21765/619196297.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model.evaluate_generator(val_generator,verbose =0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4457930326461792, 0.8611806035041809]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(val_generator,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21765/984775710.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model.evaluate_generator(test_generator,verbose = 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4306773245334625, 0.8673316836357117]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_generator,verbose = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probamos lo mismo pero eliminando free items y sporting goods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_sin = \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/data_sin/\"\n",
    "output_folder_sin = \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/split_sin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with a ratio.\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio(input_folder_sin, output=output_folder_sin,\n",
    "                    seed=42,\n",
    "                    ratio=(.7, .2, .1), #usamos 70% train, 20% val, 10% test \n",
    "                    group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13963 images belonging to 5 classes.\n",
      "Found 3989 images belonging to 5 classes.\n",
      "Found 1998 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen_sin = ImageDataGenerator(rescale = 1./255)\n",
    "val_datagen_sin = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen_sin = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#creamos los generadores con batch de 30 fotos con el dir correpondiente\n",
    "train_dir_sin= \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/split_sin/train\"\n",
    "val_dir_sin = \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/split_sin/val\"\n",
    "test_dir_sin= \"/home/vvdiaz1/code/vvdiaz1/automatic_tagging/raw_data/split_sin/test\"\n",
    "\n",
    "train_generator_sin = train_datagen_sin.flow_from_directory(\n",
    "                                train_dir_sin,\n",
    "                                target_size =(480, 360), # target_size = input image size\n",
    "                                batch_size = 5,\n",
    "                                class_mode ='categorical')\n",
    "\n",
    "val_generator_sin = val_datagen_sin.flow_from_directory(\n",
    "                                val_dir_sin,\n",
    "                                target_size =(480, 360), # target_size = input image size\n",
    "                                batch_size = 5,\n",
    "                                class_mode ='categorical')\n",
    "\n",
    "test_generator_sin = test_datagen_sin.flow_from_directory(\n",
    "                                test_dir_sin,\n",
    "                                target_size =(480, 360), # target_size = input image size\n",
    "                                batch_size = 5,\n",
    "                                class_mode ='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_own_model_sin():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(480,360,3), name='input'))\n",
    "    model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "    model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "    model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "                    optimizer = 'adam',\n",
    "                    metrics = 'accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21765/3258402138.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model_sin.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 28s 2s/step - loss: 1.4633 - accuracy: 0.3733 - val_loss: 1.2280 - val_accuracy: 0.5333\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 23s 1s/step - loss: 1.1803 - accuracy: 0.5467 - val_loss: 1.1067 - val_accuracy: 0.6267\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 23s 2s/step - loss: 0.9484 - accuracy: 0.6267 - val_loss: 0.6732 - val_accuracy: 0.7733\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 22s 1s/step - loss: 0.7573 - accuracy: 0.6933 - val_loss: 0.7530 - val_accuracy: 0.7200\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.9608 - accuracy: 0.5733 - val_loss: 0.9499 - val_accuracy: 0.6133\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 23s 2s/step - loss: 0.5592 - accuracy: 0.8000 - val_loss: 0.5638 - val_accuracy: 0.7867\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 23s 2s/step - loss: 0.7454 - accuracy: 0.7067 - val_loss: 0.5618 - val_accuracy: 0.8267\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 28s 2s/step - loss: 0.6226 - accuracy: 0.7733 - val_loss: 0.5982 - val_accuracy: 0.7600\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.7397 - accuracy: 0.7200 - val_loss: 0.8190 - val_accuracy: 0.6933\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 27s 2s/step - loss: 0.6910 - accuracy: 0.7200 - val_loss: 0.6709 - val_accuracy: 0.7333\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.5825 - accuracy: 0.8267 - val_loss: 0.4728 - val_accuracy: 0.8533\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 23s 2s/step - loss: 0.5188 - accuracy: 0.8000 - val_loss: 0.5179 - val_accuracy: 0.8667\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 24s 2s/step - loss: 0.5269 - accuracy: 0.7867 - val_loss: 0.4280 - val_accuracy: 0.9067\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.4693 - accuracy: 0.8533 - val_loss: 0.4460 - val_accuracy: 0.8533\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 22s 1s/step - loss: 0.5070 - accuracy: 0.8400 - val_loss: 0.4032 - val_accuracy: 0.8400\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 23s 2s/step - loss: 0.2823 - accuracy: 0.9200 - val_loss: 0.3589 - val_accuracy: 0.8133\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.4431 - accuracy: 0.8267 - val_loss: 0.3180 - val_accuracy: 0.8800\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 27s 2s/step - loss: 0.3132 - accuracy: 0.8667 - val_loss: 0.4996 - val_accuracy: 0.8000\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.4024 - accuracy: 0.8267 - val_loss: 0.3526 - val_accuracy: 0.8667\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 33s 2s/step - loss: 0.3736 - accuracy: 0.9067 - val_loss: 0.2631 - val_accuracy: 0.9067\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 25s 2s/step - loss: 0.2982 - accuracy: 0.9200 - val_loss: 0.3122 - val_accuracy: 0.8933\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 24s 2s/step - loss: 0.3760 - accuracy: 0.8800 - val_loss: 0.3417 - val_accuracy: 0.8800\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 24s 2s/step - loss: 0.3209 - accuracy: 0.9067 - val_loss: 0.2680 - val_accuracy: 0.9200\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 24s 2s/step - loss: 0.4239 - accuracy: 0.8400 - val_loss: 0.4238 - val_accuracy: 0.8000\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.4386 - accuracy: 0.7867 - val_loss: 0.5805 - val_accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "model_sin = load_own_model_sin()\n",
    "history = model_sin.fit_generator(\n",
    "             train_generator_sin,\n",
    "             steps_per_epoch = 15,\n",
    "             epochs = 30,\n",
    "             callbacks = EarlyStopping(patience = 5, restore_best_weights=True),\n",
    "             validation_data = val_generator_sin,\n",
    "             validation_steps = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21765/2702451891.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_sin.evaluate_generator(val_generator_sin,verbose = 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34655869007110596, 0.8846828937530518]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sin.evaluate_generator(val_generator_sin,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21765/2449133228.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model_sin.evaluate_generator(test_generator_sin,verbose = 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34003281593322754, 0.8823823928833008]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sin.evaluate_generator(test_generator_sin,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automatic-tagging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Dec 20 2022, 21:33:07) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f29be12947edb0fb12a85f03ec93c6b19442e375d04585e5cfb5d674835b1a2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
