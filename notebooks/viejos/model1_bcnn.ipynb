{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias\n",
    "import keras\n",
    "import numpy as np\n",
    "# import os\n",
    "# from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.initializers import he_normal\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- master category classes ---\n",
    "coarse1_classes = 7\n",
    "#--- coarse 2 classes ---\n",
    "coarse2_classes = 45\n",
    "#--- fine classes ---\n",
    "num_classes  = 142\n",
    "\n",
    "alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- data loading ----------------------\n",
    "# X = llamar al X de fede\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "x_train = np.reshape(train[b'data'], (train_size, channel, height, width)).transpose(0, 2, 3, 1).astype(\"float32\")\n",
    "x_train = (x_train-np.mean(x_train)) / np.std(x_train)\n",
    "\n",
    "x_test = np.reshape(test[b'data'], (test_size, channel, height, width)).transpose(0, 2, 3, 1).astype(\"float32\")\n",
    "x_test = (x_test-np.mean(x_test)) / np.std(x_test)\n",
    "\n",
    "y_train = np.zeros((train_size, num_classes)).astype('float32')\n",
    "y_c2_train = np.zeros((train_size, coarse2_classes)).astype('float32')\n",
    "\n",
    "y_test = np.zeros((test_size, num_classes)).astype('float32')\n",
    "y_c2_test = np.zeros((test_size, coarse2_classes)).astype('float32')\n",
    "\n",
    "y_train[np.arange(train_size), train[b'fine_labels']] = 1\n",
    "y_c2_train[np.arange(train_size), train[b'coarse_labels']] = 1\n",
    "\n",
    "y_test[np.arange(test_size), test[b'fine_labels']] = 1\n",
    "y_c2_test[np.arange(test_size), test[b'coarse_labels']] = 1\n",
    "\n",
    "c2_to_f = np.zeros((coarse2_classes, num_classes)).astype('float32')\n",
    "fine_unique, fine_unique_indices = np.unique(train[b'fine_labels'], return_index=True)\n",
    "for i in fine_unique_indices:\n",
    "  c2_to_f[train[b'coarse_labels'][i]][train[b'fine_labels'][i]] = 1\n",
    "\n",
    "parent_c2 = {\n",
    "  0:0, 1:0, 2:1, 3:2, \n",
    "  4:1, 5:2, 6:2, 7:3, \n",
    "  8:4, 9:5, 10:5, 11:4, \n",
    "  12:4, 13:3, 14:6, 15:4, \n",
    "  16:4, 17:1, 18:7, 19:7\n",
    "}\n",
    "\n",
    "y_c1_train = np.zeros((y_c2_train.shape[0], coarse1_classes)).astype(\"float32\")\n",
    "y_c1_test = np.zeros((y_c2_test.shape[0], coarse1_classes)).astype(\"float32\")\n",
    "for i in range(y_c1_train.shape[0]):\n",
    "  y_c1_train[i][parent_c2[np.argmax(y_c2_train[i])]] = 1.0\n",
    "for i in range(y_c1_test.shape[0]):\n",
    "  y_c1_test[i][parent_c2[np.argmax(y_c2_test[i])]] = 1.0\n",
    "\n",
    "del(train)\n",
    "del(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bcnn_model(X):\n",
    "    #input\n",
    "    img_input = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    #--- block 1 ---\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    #--- block 2 ---\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    #--- master category ---\n",
    "    c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "    c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "    c_1_bch = BatchNormalization()(c_1_bch)\n",
    "    c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "    c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "    c_1_bch = BatchNormalization()(c_1_bch)\n",
    "    c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "    c_1_pred = Dense(coarse1_classes, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "    \n",
    "    #--- block 3 ---\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    #--- subcategory ---\n",
    "    c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "    c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch)\n",
    "    c_2_bch = BatchNormalization()(c_2_bch)\n",
    "    c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "    c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "    c_2_bch = BatchNormalization()(c_2_bch)\n",
    "    c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "    c_2_pred = Dense(coarse2_classes, activation='softmax', name='c2_predictions_cifar100')(c_2_bch)\n",
    "\n",
    "    #--- block 4 ---\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    #--- block 5 ---\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    #--- articly type ---\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc_cifar100_1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(4096, activation='relu', name='fc_cifar100_2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar100')(x)\n",
    "\n",
    "    model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='vgg16_hierarchy')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bcnn_model()\n",
    "# model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_bcnn_model(model):\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                optimizer= optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True), \n",
    "                loss_weights=[alpha, beta, gamma], \n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    # tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "    # change_lr = LearningRateScheduler(scheduler)\n",
    "    change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "    # cbks = [change_lr, tb_cb, change_lw]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                X,y,\n",
    "                batch_size = 128,\n",
    "                epochs = 80\n",
    "                patience = 2, \n",
    "                restore_best_weights = True, \n",
    "                verbose = 0):\n",
    "    \n",
    "    es = EarlyStopping(monitor=\"val_loss\",\n",
    "                    patience=patience,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=0)\n",
    "    \n",
    "    history = model.fit(x_train, [y_c1_train, y_c2_train, y_c3_train],\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose= verbose,\n",
    "            callbacks = es,\n",
    "            # callbacks=cbks,\n",
    "            validation_data=(x_test, [y_c1_test, y_c2_test, y_c3_test]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax', name='c2_predictions_cifar100')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar100')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='vgg16_hierarchy')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "cbks = [change_lr, tb_cb, change_lw]\n",
    "\n",
    "model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# The following compile() is just a behavior to make sure this model can be saved.\n",
    "# We thought it may be a bug of Keras which cannot save a model compiled with loss_weights parameter\n",
    "#---------------------------------------------------------------------------------\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              optimizer=sgd, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.save(model_path)\n",
    "score = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "print('score is: ', score)\n",
    "Footer\n",
    "Â© 2023 GitHub, Inc.\n",
    "Footer navigation\n",
    "Terms\n",
    "Privacy\n",
    "Security\n",
    "Status\n",
    "Docs\n",
    "Contact GitHub\n",
    "Pricing\n",
    "API\n",
    "Training\n",
    "Blog\n",
    "About\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 14:31:41) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b766fd3b13a0d26155c4f537c73e84d8eae09f0b10c7fe7a8fb4eec6202f4718"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
